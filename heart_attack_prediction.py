# -*- coding: utf-8 -*-
"""Heart-Attack-Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iAbWIuvZ4EKEPeGhWzGnQmfffCKqxtsQ
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
pd.options.display.float_format = '{:.2f}'.format
import warnings
warnings.filterwarnings('ignore')

data = pd.read_csv('/content/heart_DS.csv')
data.head()

data.shape

data.info() #Information about the Dataset

data.columns

sns.heatmap(data.isnull(),cmap = 'magma',cbar = False); #No null values present in the data

data.describe().T  #describes transposed statistical summary of the numeric columns in the DataFrame

data.hist()

targ = data["HeartDisease"]
sns.countplot(x=targ, data=data, palette="Set2");

numeric_data = data.select_dtypes(include=['number'])  # Select only numeric columns
plt.figure(figsize=(13,13))
sns.heatmap(numeric_data.corr(), annot=True, cmap='terrain')

#Mean values of all the features for cases of heart diseases and non-heart diseases.

yes = data[data['HeartDisease'] == 1].describe().T
no = data[data['HeartDisease'] == 0].describe().T
colors = ['#F93822','#FDD20E']

fig,ax = plt.subplots(nrows = 1,ncols = 2,figsize = (5,5))
plt.subplot(1,2,1)
sns.heatmap(yes[['mean']],annot = True,cmap = colors,linewidths = 0.4,linecolor = 'black',cbar = False,fmt = '.2f',)
plt.title('Heart Disease');

plt.subplot(1,2,2)
sns.heatmap(no[['mean']],annot = True,cmap = colors,linewidths = 0.4,linecolor = 'black',cbar = False,fmt = '.2f')
plt.title('No Heart Disease');

fig.tight_layout(pad = 2)

"""**Logistic Regression**"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Convert categorical columns to numerical values using Label Encoding
label_encoder = LabelEncoder()

data['Sex'] = label_encoder.fit_transform(data['Sex'])  # Male=1, Female=0
data['ChestPainType'] = label_encoder.fit_transform(data['ChestPainType'])
data['RestingECG'] = label_encoder.fit_transform(data['RestingECG'])
data['ExerciseAngina'] = label_encoder.fit_transform(data['ExerciseAngina'])
data['ST_Slope'] = label_encoder.fit_transform(data['ST_Slope'])

# Features (X) and Target (y)
X = data.drop('HeartDisease', axis=1)
y = data['HeartDisease']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) #Split the data into training and testing sets

log_reg = LogisticRegression()
log_reg.fit(X_train, y_train) #training data
y_pred = log_reg.predict(X_test) #Predict the testing data

# Evaluate the performance
accuracy = accuracy_score(y_test, y_pred)
conf_matrix = confusion_matrix(y_test, y_pred)
class_report = classification_report(y_test, y_pred)

print(f'Accuracy: {accuracy}')
print(f'Confusion Matrix:\n{conf_matrix}')
print(f'Classification Report:\n{class_report}')

"""**Support Vector Classifier**"""

from sklearn.svm import SVC
svc_scores = []
kernels = ['linear', 'poly', 'rbf', 'sigmoid']
for i in range(len(kernels)):
    svc_classifier = SVC(kernel = kernels[i])
    svc_classifier.fit(X_train, y_train)
    svc_scores.append(svc_classifier.score(X_test, y_test))

plt.bar(kernels, svc_scores, color = colors)
for i in range(len(kernels)):
    plt.text(i, svc_scores[i], svc_scores[i])
plt.xlabel('Kernels')
plt.ylabel('Scores')
plt.title('Support Vector Classifier scores for different kernels')

print("The score for Support Vector Classifier is {}% with {} kernel.".format(svc_scores[0]*100, 'linear'))

"""**Decision Tree Classifier**"""

from sklearn.tree import DecisionTreeClassifier
classifier_dt = DecisionTreeClassifier(random_state = 1000,max_depth = 4,min_samples_leaf = 1)

classifier_dt.fit(X_train,y_train)
y_pred_dt = classifier_dt.predict(X_test)

dt_scores = []
for i in range(1, len(X.columns) + 1):
    dt_classifier = DecisionTreeClassifier(max_features = i, random_state = 0)
    dt_classifier.fit(X_train, y_train)
    dt_scores.append(dt_classifier.score(X_test, y_test))
plt.plot([i for i in range(1, len(X.columns) + 1)], dt_scores, color = 'green')
for i in range(1, len(X.columns) + 1):
    plt.text(i, dt_scores[i-1], (i, dt_scores[i-1]))
plt.xticks([i for i in range(1, len(X.columns) + 1)])
plt.xlabel('Max features')
plt.ylabel('Scores')
plt.title('Decision Tree Classifier scores for different number of maximum features')

# Assuming you want to get the score for 7 features
max_features_value = 7
score_max=dt_scores[max_features_value - 1]
print("Score for Decision Tree Classifier is {:.2f}% with {} maximum features.".format(score_max * 100, max_features_value))

"""**KNN Model**"""

from sklearn.neighbors import KNeighborsClassifier
classifier_knn = KNeighborsClassifier(n_neighbors=5)
classifier_knn.fit(X_train, y_train)
y_pred_knn = classifier_knn.predict(X_test)
knn_scores = []#List to store accuracy scores for different values of k (1 to 20)
for k in range(1, 21):  # Assigning for k=1 to 20
    knn_classifier = KNeighborsClassifier(n_neighbors=k)
    knn_classifier.fit(X_train, y_train)
    knn_scores.append(knn_classifier.score(X_test, y_test))

plt.plot([k for k in range(1, 21)], knn_scores, color='blue')
for k in range(1, 21):
    plt.text(k, knn_scores[k-1], f"({k}, {knn_scores[k-1]:.2f})")
# Labeling the plot
plt.xticks([k for k in range(1, 21)])
plt.xlabel('Number of Neighbors (k)')
plt.ylabel('Accuracy')
plt.title('KNN Classifier Accuracy for Different k Values')
plt.show()
print(f"Score for KNN model: {accuracy * 100:.2f}%")

"""**Gradient Boosting Classifier**"""

from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score
classifier_gb = GradientBoostingClassifier(random_state=1000)
classifier_gb.fit(X_train, y_train)
y_pred_gb = classifier_gb.predict(X_test) #Predict the testing data

accuracy_gb = classifier_gb.score(X_test, y_test) #Calculate accuracy
print(f"Overall accuracy of Gradient Boosting model: {accuracy_gb * 100:.2f}%")

